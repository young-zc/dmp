2019-01-21 15:19:51,319 [main] [org.apache.spark.SparkContext] [INFO] - Running Spark version 2.2.2
2019-01-21 15:19:51,319-[TS] INFO main org.apache.spark.SparkContext - Running Spark version 2.2.2
2019-01-21 15:19:51,732 [main] [org.apache.hadoop.util.NativeCodeLoader] [WARN] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-01-21 15:19:51,732-[TS] WARN main org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-01-21 15:19:56,871 [main] [org.apache.spark.SparkContext] [INFO] - Submitted application: DMPMain$
2019-01-21 15:19:56,871-[TS] INFO main org.apache.spark.SparkContext - Submitted application: DMPMain$
2019-01-21 15:19:56,886 [main] [org.apache.spark.SecurityManager] [INFO] - Changing view acls to: newforesee
2019-01-21 15:19:56,886-[TS] INFO main org.apache.spark.SecurityManager - Changing view acls to: newforesee
2019-01-21 15:19:56,886 [main] [org.apache.spark.SecurityManager] [INFO] - Changing modify acls to: newforesee
2019-01-21 15:19:56,886-[TS] INFO main org.apache.spark.SecurityManager - Changing modify acls to: newforesee
2019-01-21 15:19:56,887 [main] [org.apache.spark.SecurityManager] [INFO] - Changing view acls groups to: 
2019-01-21 15:19:56,887-[TS] INFO main org.apache.spark.SecurityManager - Changing view acls groups to: 
2019-01-21 15:19:56,887 [main] [org.apache.spark.SecurityManager] [INFO] - Changing modify acls groups to: 
2019-01-21 15:19:56,887-[TS] INFO main org.apache.spark.SecurityManager - Changing modify acls groups to: 
2019-01-21 15:19:56,887 [main] [org.apache.spark.SecurityManager] [INFO] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(newforesee); groups with view permissions: Set(); users  with modify permissions: Set(newforesee); groups with modify permissions: Set()
2019-01-21 15:19:56,887-[TS] INFO main org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(newforesee); groups with view permissions: Set(); users  with modify permissions: Set(newforesee); groups with modify permissions: Set()
2019-01-21 15:19:57,103 [main] [org.apache.spark.util.Utils] [INFO] - Successfully started service 'sparkDriver' on port 53077.
2019-01-21 15:19:57,103-[TS] INFO main org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 53077.
2019-01-21 15:19:57,118 [main] [org.apache.spark.SparkEnv] [INFO] - Registering MapOutputTracker
2019-01-21 15:19:57,118-[TS] INFO main org.apache.spark.SparkEnv - Registering MapOutputTracker
2019-01-21 15:19:57,131 [main] [org.apache.spark.SparkEnv] [INFO] - Registering BlockManagerMaster
2019-01-21 15:19:57,131-[TS] INFO main org.apache.spark.SparkEnv - Registering BlockManagerMaster
2019-01-21 15:19:57,133 [main] [org.apache.spark.storage.BlockManagerMasterEndpoint] [INFO] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-01-21 15:19:57,133-[TS] INFO main org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-01-21 15:19:57,133 [main] [org.apache.spark.storage.BlockManagerMasterEndpoint] [INFO] - BlockManagerMasterEndpoint up
2019-01-21 15:19:57,133-[TS] INFO main org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2019-01-21 15:19:57,140 [main] [org.apache.spark.storage.DiskBlockManager] [INFO] - Created local directory at /private/var/folders/gw/182hsxpd1l78ny7x0627rr1m0000gn/T/blockmgr-d34fa366-ebc3-4933-af5c-404220b5c0a5
2019-01-21 15:19:57,140-[TS] INFO main org.apache.spark.storage.DiskBlockManager - Created local directory at /private/var/folders/gw/182hsxpd1l78ny7x0627rr1m0000gn/T/blockmgr-d34fa366-ebc3-4933-af5c-404220b5c0a5
2019-01-21 15:19:57,180 [main] [org.apache.spark.storage.memory.MemoryStore] [INFO] - MemoryStore started with capacity 1458.6 MB
2019-01-21 15:19:57,180-[TS] INFO main org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1458.6 MB
2019-01-21 15:19:57,227 [main] [org.apache.spark.SparkEnv] [INFO] - Registering OutputCommitCoordinator
2019-01-21 15:19:57,227-[TS] INFO main org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2019-01-21 15:19:57,293 [main] [org.spark_project.jetty.util.log] [INFO] - Logging initialized @6657ms
2019-01-21 15:19:57,293-[TS] INFO main org.spark_project.jetty.util.log - Logging initialized @6657ms
2019-01-21 15:19:57,341 [main] [org.spark_project.jetty.server.Server] [INFO] - jetty-9.3.z-SNAPSHOT
2019-01-21 15:19:57,341-[TS] INFO main org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
2019-01-21 15:19:57,353 [main] [org.spark_project.jetty.server.Server] [INFO] - Started @6718ms
2019-01-21 15:19:57,353-[TS] INFO main org.spark_project.jetty.server.Server - Started @6718ms
2019-01-21 15:19:57,369 [main] [org.spark_project.jetty.server.AbstractConnector] [INFO] - Started ServerConnector@6e9319f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-01-21 15:19:57,369-[TS] INFO main org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@6e9319f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-01-21 15:19:57,369 [main] [org.apache.spark.util.Utils] [INFO] - Successfully started service 'SparkUI' on port 4040.
2019-01-21 15:19:57,369-[TS] INFO main org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2019-01-21 15:19:57,390 [main] [org.spark_project.jetty.server.handler.ContextHandler] [INFO] - Started o.s.j.s.ServletContextHandler@7f811d00{/jobs,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,390-[TS] INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f811d00{/jobs,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,391 [main] [org.spark_project.jetty.server.handler.ContextHandler] [INFO] - Started o.s.j.s.ServletContextHandler@a486d78{/jobs/json,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,391-[TS] INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a486d78{/jobs/json,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,391 [main] [org.spark_project.jetty.server.handler.ContextHandler] [INFO] - Started o.s.j.s.ServletContextHandler@7ef2d7a6{/jobs/job,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,391-[TS] INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ef2d7a6{/jobs/job,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,393 [main] [org.spark_project.jetty.server.handler.ContextHandler] [INFO] - Started o.s.j.s.ServletContextHandler@21526f6c{/jobs/job/json,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,393-[TS] INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21526f6c{/jobs/job/json,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,394 [main] [org.spark_project.jetty.server.handler.ContextHandler] [INFO] - Started o.s.j.s.ServletContextHandler@299266e2{/stages,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,394-[TS] INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@299266e2{/stages,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,394 [main] [org.spark_project.jetty.server.handler.ContextHandler] [INFO] - Started o.s.j.s.ServletContextHandler@66ea1466{/stages/json,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,394-[TS] INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66ea1466{/stages/json,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,395 [main] [org.spark_project.jetty.server.handler.ContextHandler] [INFO] - Started o.s.j.s.ServletContextHandler@3bffddff{/stages/stage,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,395-[TS] INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3bffddff{/stages/stage,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,397 [main] [org.spark_project.jetty.server.handler.ContextHandler] [INFO] - Started o.s.j.s.ServletContextHandler@142eef62{/stages/stage/json,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,397-[TS] INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@142eef62{/stages/stage/json,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,398 [main] [org.spark_project.jetty.server.handler.ContextHandler] [INFO] - Started o.s.j.s.ServletContextHandler@5990e6c5{/stages/pool,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,398-[TS] INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5990e6c5{/stages/pool,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,399 [main] [org.spark_project.jetty.server.handler.ContextHandler] [INFO] - Started o.s.j.s.ServletContextHandler@35d6ca49{/stages/pool/json,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,399-[TS] INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35d6ca49{/stages/pool/json,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,399 [main] [org.spark_project.jetty.server.handler.ContextHandler] [INFO] - Started o.s.j.s.ServletContextHandler@47289387{/storage,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,399-[TS] INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47289387{/storage,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,400 [main] [org.spark_project.jetty.server.handler.ContextHandler] [INFO] - Started o.s.j.s.ServletContextHandler@114a85c2{/storage/json,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,400-[TS] INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@114a85c2{/storage/json,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,400 [main] [org.spark_project.jetty.server.handler.ContextHandler] [INFO] - Started o.s.j.s.ServletContextHandler@cf65451{/storage/rdd,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,400-[TS] INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cf65451{/storage/rdd,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,401 [main] [org.spark_project.jetty.server.handler.ContextHandler] [INFO] - Started o.s.j.s.ServletContextHandler@37eeec90{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,401-[TS] INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37eeec90{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,402 [main] [org.spark_project.jetty.server.handler.ContextHandler] [INFO] - Started o.s.j.s.ServletContextHandler@c9413d8{/environment,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,402-[TS] INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c9413d8{/environment,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,403 [main] [org.spark_project.jetty.server.handler.ContextHandler] [INFO] - Started o.s.j.s.ServletContextHandler@46074492{/environment/json,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,403-[TS] INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46074492{/environment/json,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,404 [main] [org.spark_project.jetty.server.handler.ContextHandler] [INFO] - Started o.s.j.s.ServletContextHandler@2c715e84{/executors,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,404-[TS] INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c715e84{/executors,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,405 [main] [org.spark_project.jetty.server.handler.ContextHandler] [INFO] - Started o.s.j.s.ServletContextHandler@3b9d6699{/executors/json,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,405-[TS] INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b9d6699{/executors/json,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,406 [main] [org.spark_project.jetty.server.handler.ContextHandler] [INFO] - Started o.s.j.s.ServletContextHandler@21694e53{/executors/threadDump,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,406-[TS] INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21694e53{/executors/threadDump,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,407 [main] [org.spark_project.jetty.server.handler.ContextHandler] [INFO] - Started o.s.j.s.ServletContextHandler@22c86919{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,407-[TS] INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22c86919{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,415 [main] [org.spark_project.jetty.server.handler.ContextHandler] [INFO] - Started o.s.j.s.ServletContextHandler@1b0a7baf{/static,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,415-[TS] INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b0a7baf{/static,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,416 [main] [org.spark_project.jetty.server.handler.ContextHandler] [INFO] - Started o.s.j.s.ServletContextHandler@24f360b2{/,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,416-[TS] INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24f360b2{/,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,417 [main] [org.spark_project.jetty.server.handler.ContextHandler] [INFO] - Started o.s.j.s.ServletContextHandler@302fec27{/api,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,417-[TS] INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@302fec27{/api,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,418 [main] [org.spark_project.jetty.server.handler.ContextHandler] [INFO] - Started o.s.j.s.ServletContextHandler@6cea706c{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,418-[TS] INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6cea706c{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,419 [main] [org.spark_project.jetty.server.handler.ContextHandler] [INFO] - Started o.s.j.s.ServletContextHandler@2f2bf0e2{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,419-[TS] INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2f2bf0e2{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,422 [main] [org.apache.spark.ui.SparkUI] [INFO] - Bound SparkUI to 0.0.0.0, and started at http://10.0.157.188:4040
2019-01-21 15:19:57,422-[TS] INFO main org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://10.0.157.188:4040
2019-01-21 15:19:57,489 [main] [org.apache.spark.executor.Executor] [INFO] - Starting executor ID driver on host localhost
2019-01-21 15:19:57,489-[TS] INFO main org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
2019-01-21 15:19:57,506 [main] [org.apache.spark.util.Utils] [INFO] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53078.
2019-01-21 15:19:57,506-[TS] INFO main org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53078.
2019-01-21 15:19:57,506 [main] [org.apache.spark.network.netty.NettyBlockTransferService] [INFO] - Server created on 10.0.157.188:53078
2019-01-21 15:19:57,506-[TS] INFO main org.apache.spark.network.netty.NettyBlockTransferService - Server created on 10.0.157.188:53078
2019-01-21 15:19:57,507 [main] [org.apache.spark.storage.BlockManager] [INFO] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-01-21 15:19:57,507-[TS] INFO main org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-01-21 15:19:57,525 [main] [org.apache.spark.storage.BlockManagerMaster] [INFO] - Registering BlockManager BlockManagerId(driver, 10.0.157.188, 53078, None)
2019-01-21 15:19:57,525-[TS] INFO main org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 10.0.157.188, 53078, None)
2019-01-21 15:19:57,528 [dispatcher-event-loop-2] [org.apache.spark.storage.BlockManagerMasterEndpoint] [INFO] - Registering block manager 10.0.157.188:53078 with 1458.6 MB RAM, BlockManagerId(driver, 10.0.157.188, 53078, None)
2019-01-21 15:19:57,528-[TS] INFO dispatcher-event-loop-2 org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 10.0.157.188:53078 with 1458.6 MB RAM, BlockManagerId(driver, 10.0.157.188, 53078, None)
2019-01-21 15:19:57,530 [main] [org.apache.spark.storage.BlockManagerMaster] [INFO] - Registered BlockManager BlockManagerId(driver, 10.0.157.188, 53078, None)
2019-01-21 15:19:57,530-[TS] INFO main org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 10.0.157.188, 53078, None)
2019-01-21 15:19:57,531 [main] [org.apache.spark.storage.BlockManager] [INFO] - Initialized BlockManager: BlockManagerId(driver, 10.0.157.188, 53078, None)
2019-01-21 15:19:57,531-[TS] INFO main org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 10.0.157.188, 53078, None)
2019-01-21 15:19:57,735 [main] [org.spark_project.jetty.server.handler.ContextHandler] [INFO] - Started o.s.j.s.ServletContextHandler@4ee33af7{/metrics/json,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,735-[TS] INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ee33af7{/metrics/json,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,794 [main] [org.apache.spark.sql.internal.SharedState] [INFO] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/newforesee/Intellij%20Project/DMP/spark-warehouse').
2019-01-21 15:19:57,794-[TS] INFO main org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/newforesee/Intellij%20Project/DMP/spark-warehouse').
2019-01-21 15:19:57,794 [main] [org.apache.spark.sql.internal.SharedState] [INFO] - Warehouse path is 'file:/Users/newforesee/Intellij%20Project/DMP/spark-warehouse'.
2019-01-21 15:19:57,794-[TS] INFO main org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/Users/newforesee/Intellij%20Project/DMP/spark-warehouse'.
2019-01-21 15:19:57,799 [main] [org.spark_project.jetty.server.handler.ContextHandler] [INFO] - Started o.s.j.s.ServletContextHandler@69b2f8e5{/SQL,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,799-[TS] INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69b2f8e5{/SQL,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,800 [main] [org.spark_project.jetty.server.handler.ContextHandler] [INFO] - Started o.s.j.s.ServletContextHandler@a10c1b5{/SQL/json,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,800-[TS] INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a10c1b5{/SQL/json,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,800 [main] [org.spark_project.jetty.server.handler.ContextHandler] [INFO] - Started o.s.j.s.ServletContextHandler@4d847d32{/SQL/execution,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,800-[TS] INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d847d32{/SQL/execution,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,801 [main] [org.spark_project.jetty.server.handler.ContextHandler] [INFO] - Started o.s.j.s.ServletContextHandler@3d7fa3ae{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,801-[TS] INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d7fa3ae{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,802 [main] [org.spark_project.jetty.server.handler.ContextHandler] [INFO] - Started o.s.j.s.ServletContextHandler@6ad11a56{/static/sql,null,AVAILABLE,@Spark}
2019-01-21 15:19:57,802-[TS] INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ad11a56{/static/sql,null,AVAILABLE,@Spark}
2019-01-21 15:19:58,330 [main] [org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef] [INFO] - Registered StateStoreCoordinator endpoint
2019-01-21 15:19:58,330-[TS] INFO main org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2019-01-21 15:19:58,822 [main] [org.apache.spark.SparkContext] [INFO] - Starting job: parquet at DMPMain.scala:52
2019-01-21 15:19:58,822-[TS] INFO main org.apache.spark.SparkContext - Starting job: parquet at DMPMain.scala:52
2019-01-21 15:19:58,836 [dag-scheduler-event-loop] [org.apache.spark.scheduler.DAGScheduler] [INFO] - Got job 0 (parquet at DMPMain.scala:52) with 1 output partitions
2019-01-21 15:19:58,836-[TS] INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Got job 0 (parquet at DMPMain.scala:52) with 1 output partitions
2019-01-21 15:19:58,837 [dag-scheduler-event-loop] [org.apache.spark.scheduler.DAGScheduler] [INFO] - Final stage: ResultStage 0 (parquet at DMPMain.scala:52)
2019-01-21 15:19:58,837-[TS] INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (parquet at DMPMain.scala:52)
2019-01-21 15:19:58,837 [dag-scheduler-event-loop] [org.apache.spark.scheduler.DAGScheduler] [INFO] - Parents of final stage: List()
2019-01-21 15:19:58,837-[TS] INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
2019-01-21 15:19:58,838 [dag-scheduler-event-loop] [org.apache.spark.scheduler.DAGScheduler] [INFO] - Missing parents: List()
2019-01-21 15:19:58,838-[TS] INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2019-01-21 15:19:58,841 [dag-scheduler-event-loop] [org.apache.spark.scheduler.DAGScheduler] [INFO] - Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at DMPMain.scala:52), which has no missing parents
2019-01-21 15:19:58,841-[TS] INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at DMPMain.scala:52), which has no missing parents
2019-01-21 15:19:58,886 [dag-scheduler-event-loop] [org.apache.spark.storage.memory.MemoryStore] [INFO] - Block broadcast_0 stored as values in memory (estimated size 69.3 KB, free 1458.5 MB)
2019-01-21 15:19:58,886-[TS] INFO dag-scheduler-event-loop org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 69.3 KB, free 1458.5 MB)
2019-01-21 15:19:59,028 [dag-scheduler-event-loop] [org.apache.spark.storage.memory.MemoryStore] [INFO] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 24.4 KB, free 1458.5 MB)
2019-01-21 15:19:59,028-[TS] INFO dag-scheduler-event-loop org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 24.4 KB, free 1458.5 MB)
2019-01-21 15:19:59,031 [dispatcher-event-loop-5] [org.apache.spark.storage.BlockManagerInfo] [INFO] - Added broadcast_0_piece0 in memory on 10.0.157.188:53078 (size: 24.4 KB, free: 1458.6 MB)
2019-01-21 15:19:59,031-[TS] INFO dispatcher-event-loop-5 org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 10.0.157.188:53078 (size: 24.4 KB, free: 1458.6 MB)
2019-01-21 15:19:59,033 [dag-scheduler-event-loop] [org.apache.spark.SparkContext] [INFO] - Created broadcast 0 from broadcast at DAGScheduler.scala:1015
2019-01-21 15:19:59,033-[TS] INFO dag-scheduler-event-loop org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1015
2019-01-21 15:19:59,047 [dag-scheduler-event-loop] [org.apache.spark.scheduler.DAGScheduler] [INFO] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at DMPMain.scala:52) (first 15 tasks are for partitions Vector(0))
2019-01-21 15:19:59,047-[TS] INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at DMPMain.scala:52) (first 15 tasks are for partitions Vector(0))
2019-01-21 15:19:59,048 [dag-scheduler-event-loop] [org.apache.spark.scheduler.TaskSchedulerImpl] [INFO] - Adding task set 0.0 with 1 tasks
2019-01-21 15:19:59,048-[TS] INFO dag-scheduler-event-loop org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
2019-01-21 15:19:59,087 [dispatcher-event-loop-6] [org.apache.spark.scheduler.TaskSetManager] [INFO] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4867 bytes)
2019-01-21 15:19:59,087-[TS] INFO dispatcher-event-loop-6 org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4867 bytes)
2019-01-21 15:19:59,096 [Executor task launch worker for task 0] [org.apache.spark.executor.Executor] [INFO] - Running task 0.0 in stage 0.0 (TID 0)
2019-01-21 15:19:59,096-[TS] INFO Executor task launch worker for task 0 org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2019-01-21 15:19:59,414 [Executor task launch worker for task 0] [org.apache.spark.executor.Executor] [INFO] - Finished task 0.0 in stage 0.0 (TID 0). 977 bytes result sent to driver
2019-01-21 15:19:59,414-[TS] INFO Executor task launch worker for task 0 org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 977 bytes result sent to driver
2019-01-21 15:19:59,436 [task-result-getter-0] [org.apache.spark.scheduler.TaskSetManager] [INFO] - Finished task 0.0 in stage 0.0 (TID 0) in 365 ms on localhost (executor driver) (1/1)
2019-01-21 15:19:59,436-[TS] INFO task-result-getter-0 org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 365 ms on localhost (executor driver) (1/1)
2019-01-21 15:19:59,438 [task-result-getter-0] [org.apache.spark.scheduler.TaskSchedulerImpl] [INFO] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-01-21 15:19:59,438-[TS] INFO task-result-getter-0 org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-01-21 15:19:59,441 [dag-scheduler-event-loop] [org.apache.spark.scheduler.DAGScheduler] [INFO] - ResultStage 0 (parquet at DMPMain.scala:52) finished in 0.381 s
2019-01-21 15:19:59,441-[TS] INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (parquet at DMPMain.scala:52) finished in 0.381 s
2019-01-21 15:19:59,445 [main] [org.apache.spark.scheduler.DAGScheduler] [INFO] - Job 0 finished: parquet at DMPMain.scala:52, took 0.622109 s
2019-01-21 15:19:59,445-[TS] INFO main org.apache.spark.scheduler.DAGScheduler - Job 0 finished: parquet at DMPMain.scala:52, took 0.622109 s
2019-01-21 15:19:59,807 [dispatcher-event-loop-3] [org.apache.spark.storage.BlockManagerInfo] [INFO] - Removed broadcast_0_piece0 on 10.0.157.188:53078 in memory (size: 24.4 KB, free: 1458.6 MB)
2019-01-21 15:19:59,807-[TS] INFO dispatcher-event-loop-3 org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on 10.0.157.188:53078 in memory (size: 24.4 KB, free: 1458.6 MB)
2019-01-21 15:20:00,528 [main] [org.apache.spark.sql.execution.datasources.FileSourceStrategy] [INFO] - Pruning directories with: 
2019-01-21 15:20:00,528-[TS] INFO main org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: 
2019-01-21 15:20:00,531 [main] [org.apache.spark.sql.execution.datasources.FileSourceStrategy] [INFO] - Post-Scan Filters: 
2019-01-21 15:20:00,531-[TS] INFO main org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: 
2019-01-21 15:20:00,534 [main] [org.apache.spark.sql.execution.datasources.FileSourceStrategy] [INFO] - Output Data Schema: struct<value: string>
2019-01-21 15:20:00,534-[TS] INFO main org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<value: string>
2019-01-21 15:20:00,541 [main] [org.apache.spark.sql.execution.FileSourceScanExec] [INFO] - Pushed Filters: 
2019-01-21 15:20:00,541-[TS] INFO main org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: 
2019-01-21 15:20:00,822 [main] [org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator] [INFO] - Code generated in 183.022254 ms
2019-01-21 15:20:00,822-[TS] INFO main org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 183.022254 ms
2019-01-21 15:20:00,852 [main] [org.apache.spark.storage.memory.MemoryStore] [INFO] - Block broadcast_1 stored as values in memory (estimated size 267.0 KB, free 1458.3 MB)
2019-01-21 15:20:00,852-[TS] INFO main org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 267.0 KB, free 1458.3 MB)
2019-01-21 15:20:00,871 [main] [org.apache.spark.storage.memory.MemoryStore] [INFO] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 23.4 KB, free 1458.3 MB)
2019-01-21 15:20:00,871-[TS] INFO main org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 23.4 KB, free 1458.3 MB)
2019-01-21 15:20:00,872 [dispatcher-event-loop-4] [org.apache.spark.storage.BlockManagerInfo] [INFO] - Added broadcast_1_piece0 in memory on 10.0.157.188:53078 (size: 23.4 KB, free: 1458.6 MB)
2019-01-21 15:20:00,872-[TS] INFO dispatcher-event-loop-4 org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 10.0.157.188:53078 (size: 23.4 KB, free: 1458.6 MB)
2019-01-21 15:20:00,873 [main] [org.apache.spark.SparkContext] [INFO] - Created broadcast 1 from rdd at DMPMain.scala:53
2019-01-21 15:20:00,873-[TS] INFO main org.apache.spark.SparkContext - Created broadcast 1 from rdd at DMPMain.scala:53
2019-01-21 15:20:00,881 [main] [org.apache.spark.sql.execution.FileSourceScanExec] [INFO] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-01-21 15:20:00,881-[TS] INFO main org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-01-21 15:20:00,947 [main] [org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter] [INFO] - File Output Committer Algorithm version is 1
2019-01-21 15:20:00,947-[TS] INFO main org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
2019-01-21 15:20:00,971 [main] [org.apache.spark.SparkContext] [INFO] - Starting job: saveAsTextFile at DMPMain.scala:17
2019-01-21 15:20:00,971-[TS] INFO main org.apache.spark.SparkContext - Starting job: saveAsTextFile at DMPMain.scala:17
2019-01-21 15:20:00,980 [dag-scheduler-event-loop] [org.apache.spark.scheduler.DAGScheduler] [INFO] - Registering RDD 6 (map at DMPMain.scala:29)
2019-01-21 15:20:00,980-[TS] INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Registering RDD 6 (map at DMPMain.scala:29)
2019-01-21 15:20:00,980 [dag-scheduler-event-loop] [org.apache.spark.scheduler.DAGScheduler] [INFO] - Got job 1 (saveAsTextFile at DMPMain.scala:17) with 1 output partitions
2019-01-21 15:20:00,980-[TS] INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Got job 1 (saveAsTextFile at DMPMain.scala:17) with 1 output partitions
2019-01-21 15:20:00,980 [dag-scheduler-event-loop] [org.apache.spark.scheduler.DAGScheduler] [INFO] - Final stage: ResultStage 2 (saveAsTextFile at DMPMain.scala:17)
2019-01-21 15:20:00,980-[TS] INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (saveAsTextFile at DMPMain.scala:17)
2019-01-21 15:20:00,980 [dag-scheduler-event-loop] [org.apache.spark.scheduler.DAGScheduler] [INFO] - Parents of final stage: List(ShuffleMapStage 1)
2019-01-21 15:20:00,980-[TS] INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 1)
2019-01-21 15:20:00,981 [dag-scheduler-event-loop] [org.apache.spark.scheduler.DAGScheduler] [INFO] - Missing parents: List(ShuffleMapStage 1)
2019-01-21 15:20:00,981-[TS] INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 1)
2019-01-21 15:20:00,983 [dag-scheduler-event-loop] [org.apache.spark.scheduler.DAGScheduler] [INFO] - Submitting ShuffleMapStage 1 (MapPartitionsRDD[6] at map at DMPMain.scala:29), which has no missing parents
2019-01-21 15:20:00,983-[TS] INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 1 (MapPartitionsRDD[6] at map at DMPMain.scala:29), which has no missing parents
2019-01-21 15:20:01,017 [dag-scheduler-event-loop] [org.apache.spark.storage.memory.MemoryStore] [INFO] - Block broadcast_2 stored as values in memory (estimated size 13.4 KB, free 1458.3 MB)
2019-01-21 15:20:01,017-[TS] INFO dag-scheduler-event-loop org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 13.4 KB, free 1458.3 MB)
2019-01-21 15:20:01,022 [dag-scheduler-event-loop] [org.apache.spark.storage.memory.MemoryStore] [INFO] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.8 KB, free 1458.3 MB)
2019-01-21 15:20:01,022-[TS] INFO dag-scheduler-event-loop org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.8 KB, free 1458.3 MB)
2019-01-21 15:20:01,022 [dispatcher-event-loop-6] [org.apache.spark.storage.BlockManagerInfo] [INFO] - Added broadcast_2_piece0 in memory on 10.0.157.188:53078 (size: 6.8 KB, free: 1458.6 MB)
2019-01-21 15:20:01,022-[TS] INFO dispatcher-event-loop-6 org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 10.0.157.188:53078 (size: 6.8 KB, free: 1458.6 MB)
2019-01-21 15:20:01,023 [dag-scheduler-event-loop] [org.apache.spark.SparkContext] [INFO] - Created broadcast 2 from broadcast at DAGScheduler.scala:1015
2019-01-21 15:20:01,023-[TS] INFO dag-scheduler-event-loop org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1015
2019-01-21 15:20:01,025 [dag-scheduler-event-loop] [org.apache.spark.scheduler.DAGScheduler] [INFO] - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[6] at map at DMPMain.scala:29) (first 15 tasks are for partitions Vector(0))
2019-01-21 15:20:01,025-[TS] INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[6] at map at DMPMain.scala:29) (first 15 tasks are for partitions Vector(0))
2019-01-21 15:20:01,025 [dag-scheduler-event-loop] [org.apache.spark.scheduler.TaskSchedulerImpl] [INFO] - Adding task set 1.0 with 1 tasks
2019-01-21 15:20:01,025-[TS] INFO dag-scheduler-event-loop org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
2019-01-21 15:20:01,030 [dispatcher-event-loop-7] [org.apache.spark.scheduler.TaskSetManager] [INFO] - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5364 bytes)
2019-01-21 15:20:01,030-[TS] INFO dispatcher-event-loop-7 org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5364 bytes)
2019-01-21 15:20:01,030 [Executor task launch worker for task 1] [org.apache.spark.executor.Executor] [INFO] - Running task 0.0 in stage 1.0 (TID 1)
2019-01-21 15:20:01,030-[TS] INFO Executor task launch worker for task 1 org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
2019-01-21 15:20:01,068 [Executor task launch worker for task 1] [org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator] [INFO] - Code generated in 11.076728 ms
2019-01-21 15:20:01,068-[TS] INFO Executor task launch worker for task 1 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 11.076728 ms
2019-01-21 15:20:01,073 [Executor task launch worker for task 1] [org.apache.spark.sql.execution.datasources.FileScanRDD] [INFO] - Reading File path: file:///Users/newforesee/Intellij%20Project/DMP/src/main/resources/dmp/part-00000-35d057d8-3dfc-4e8b-8438-213ab702f72e-c000.snappy.parquet, range: 0-292700, partition values: [empty row]
2019-01-21 15:20:01,073-[TS] INFO Executor task launch worker for task 1 org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///Users/newforesee/Intellij%20Project/DMP/src/main/resources/dmp/part-00000-35d057d8-3dfc-4e8b-8438-213ab702f72e-c000.snappy.parquet, range: 0-292700, partition values: [empty row]
2019-01-21 15:20:01,155 [Executor task launch worker for task 1] [org.apache.hadoop.io.compress.CodecPool] [INFO] - Got brand-new decompressor [.snappy]
2019-01-21 15:20:01,155-[TS] INFO Executor task launch worker for task 1 org.apache.hadoop.io.compress.CodecPool - Got brand-new decompressor [.snappy]
2019-01-21 15:20:01,354 [Executor task launch worker for task 1] [org.apache.spark.storage.memory.MemoryStore] [INFO] - Block rdd_5_0 stored as values in memory (estimated size 1756.8 KB, free 1456.6 MB)
2019-01-21 15:20:01,354-[TS] INFO Executor task launch worker for task 1 org.apache.spark.storage.memory.MemoryStore - Block rdd_5_0 stored as values in memory (estimated size 1756.8 KB, free 1456.6 MB)
2019-01-21 15:20:01,355 [dispatcher-event-loop-2] [org.apache.spark.storage.BlockManagerInfo] [INFO] - Added rdd_5_0 in memory on 10.0.157.188:53078 (size: 1756.8 KB, free: 1456.9 MB)
2019-01-21 15:20:01,355-[TS] INFO dispatcher-event-loop-2 org.apache.spark.storage.BlockManagerInfo - Added rdd_5_0 in memory on 10.0.157.188:53078 (size: 1756.8 KB, free: 1456.9 MB)
2019-01-21 15:20:01,452 [Executor task launch worker for task 1] [org.apache.spark.executor.Executor] [INFO] - Finished task 0.0 in stage 1.0 (TID 1). 2408 bytes result sent to driver
2019-01-21 15:20:01,452-[TS] INFO Executor task launch worker for task 1 org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2408 bytes result sent to driver
2019-01-21 15:20:01,457 [task-result-getter-1] [org.apache.spark.scheduler.TaskSetManager] [INFO] - Finished task 0.0 in stage 1.0 (TID 1) in 431 ms on localhost (executor driver) (1/1)
2019-01-21 15:20:01,457-[TS] INFO task-result-getter-1 org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 431 ms on localhost (executor driver) (1/1)
2019-01-21 15:20:01,457 [task-result-getter-1] [org.apache.spark.scheduler.TaskSchedulerImpl] [INFO] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-01-21 15:20:01,457-[TS] INFO task-result-getter-1 org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-01-21 15:20:01,458 [dag-scheduler-event-loop] [org.apache.spark.scheduler.DAGScheduler] [INFO] - ShuffleMapStage 1 (map at DMPMain.scala:29) finished in 0.432 s
2019-01-21 15:20:01,458-[TS] INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 1 (map at DMPMain.scala:29) finished in 0.432 s
2019-01-21 15:20:01,459 [dag-scheduler-event-loop] [org.apache.spark.scheduler.DAGScheduler] [INFO] - looking for newly runnable stages
2019-01-21 15:20:01,459-[TS] INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
2019-01-21 15:20:01,459 [dag-scheduler-event-loop] [org.apache.spark.scheduler.DAGScheduler] [INFO] - running: Set()
2019-01-21 15:20:01,459-[TS] INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - running: Set()
2019-01-21 15:20:01,459 [dag-scheduler-event-loop] [org.apache.spark.scheduler.DAGScheduler] [INFO] - waiting: Set(ResultStage 2)
2019-01-21 15:20:01,459-[TS] INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 2)
2019-01-21 15:20:01,460 [dag-scheduler-event-loop] [org.apache.spark.scheduler.DAGScheduler] [INFO] - failed: Set()
2019-01-21 15:20:01,460-[TS] INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - failed: Set()
2019-01-21 15:20:01,462 [dag-scheduler-event-loop] [org.apache.spark.scheduler.DAGScheduler] [INFO] - Submitting ResultStage 2 (MapPartitionsRDD[9] at saveAsTextFile at DMPMain.scala:17), which has no missing parents
2019-01-21 15:20:01,462-[TS] INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[9] at saveAsTextFile at DMPMain.scala:17), which has no missing parents
2019-01-21 15:20:01,481 [dag-scheduler-event-loop] [org.apache.spark.storage.memory.MemoryStore] [INFO] - Block broadcast_3 stored as values in memory (estimated size 71.7 KB, free 1456.5 MB)
2019-01-21 15:20:01,481-[TS] INFO dag-scheduler-event-loop org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 71.7 KB, free 1456.5 MB)
2019-01-21 15:20:01,486 [dag-scheduler-event-loop] [org.apache.spark.storage.memory.MemoryStore] [INFO] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 25.8 KB, free 1456.5 MB)
2019-01-21 15:20:01,486-[TS] INFO dag-scheduler-event-loop org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 25.8 KB, free 1456.5 MB)
2019-01-21 15:20:01,486 [dispatcher-event-loop-4] [org.apache.spark.storage.BlockManagerInfo] [INFO] - Added broadcast_3_piece0 in memory on 10.0.157.188:53078 (size: 25.8 KB, free: 1456.8 MB)
2019-01-21 15:20:01,486-[TS] INFO dispatcher-event-loop-4 org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 10.0.157.188:53078 (size: 25.8 KB, free: 1456.8 MB)
2019-01-21 15:20:01,487 [dag-scheduler-event-loop] [org.apache.spark.SparkContext] [INFO] - Created broadcast 3 from broadcast at DAGScheduler.scala:1015
2019-01-21 15:20:01,487-[TS] INFO dag-scheduler-event-loop org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1015
2019-01-21 15:20:01,488 [dag-scheduler-event-loop] [org.apache.spark.scheduler.DAGScheduler] [INFO] - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at saveAsTextFile at DMPMain.scala:17) (first 15 tasks are for partitions Vector(0))
2019-01-21 15:20:01,488-[TS] INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at saveAsTextFile at DMPMain.scala:17) (first 15 tasks are for partitions Vector(0))
2019-01-21 15:20:01,488 [dag-scheduler-event-loop] [org.apache.spark.scheduler.TaskSchedulerImpl] [INFO] - Adding task set 2.0 with 1 tasks
2019-01-21 15:20:01,488-[TS] INFO dag-scheduler-event-loop org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
2019-01-21 15:20:01,491 [dispatcher-event-loop-5] [org.apache.spark.scheduler.TaskSetManager] [INFO] - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4621 bytes)
2019-01-21 15:20:01,491-[TS] INFO dispatcher-event-loop-5 org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4621 bytes)
2019-01-21 15:20:01,492 [Executor task launch worker for task 2] [org.apache.spark.executor.Executor] [INFO] - Running task 0.0 in stage 2.0 (TID 2)
2019-01-21 15:20:01,492-[TS] INFO Executor task launch worker for task 2 org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
2019-01-21 15:20:01,515 [Executor task launch worker for task 2] [org.apache.spark.storage.ShuffleBlockFetcherIterator] [INFO] - Getting 1 non-empty blocks out of 1 blocks
2019-01-21 15:20:01,515-[TS] INFO Executor task launch worker for task 2 org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
2019-01-21 15:20:01,516 [Executor task launch worker for task 2] [org.apache.spark.storage.ShuffleBlockFetcherIterator] [INFO] - Started 0 remote fetches in 4 ms
2019-01-21 15:20:01,516-[TS] INFO Executor task launch worker for task 2 org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
2019-01-21 15:20:01,553 [Executor task launch worker for task 2] [org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter] [INFO] - File Output Committer Algorithm version is 1
2019-01-21 15:20:01,553-[TS] INFO Executor task launch worker for task 2 org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
2019-01-21 15:20:01,588 [Executor task launch worker for task 2] [org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter] [INFO] - Saved output of task 'attempt_20190121152000_0002_m_000000_2' to file:/Users/newforesee/Intellij Project/DMP/src/main/resources/json/_temporary/0/task_20190121152000_0002_m_000000
2019-01-21 15:20:01,588-[TS] INFO Executor task launch worker for task 2 org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190121152000_0002_m_000000_2' to file:/Users/newforesee/Intellij Project/DMP/src/main/resources/json/_temporary/0/task_20190121152000_0002_m_000000
2019-01-21 15:20:01,589 [Executor task launch worker for task 2] [org.apache.spark.mapred.SparkHadoopMapRedUtil] [INFO] - attempt_20190121152000_0002_m_000000_2: Committed
2019-01-21 15:20:01,589-[TS] INFO Executor task launch worker for task 2 org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190121152000_0002_m_000000_2: Committed
2019-01-21 15:20:01,592 [Executor task launch worker for task 2] [org.apache.spark.executor.Executor] [INFO] - Finished task 0.0 in stage 2.0 (TID 2). 1139 bytes result sent to driver
2019-01-21 15:20:01,592-[TS] INFO Executor task launch worker for task 2 org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 1139 bytes result sent to driver
2019-01-21 15:20:01,597 [task-result-getter-2] [org.apache.spark.scheduler.TaskSetManager] [INFO] - Finished task 0.0 in stage 2.0 (TID 2) in 107 ms on localhost (executor driver) (1/1)
2019-01-21 15:20:01,597-[TS] INFO task-result-getter-2 org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 107 ms on localhost (executor driver) (1/1)
2019-01-21 15:20:01,598 [task-result-getter-2] [org.apache.spark.scheduler.TaskSchedulerImpl] [INFO] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-01-21 15:20:01,598-[TS] INFO task-result-getter-2 org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-01-21 15:20:01,599 [dag-scheduler-event-loop] [org.apache.spark.scheduler.DAGScheduler] [INFO] - ResultStage 2 (saveAsTextFile at DMPMain.scala:17) finished in 0.109 s
2019-01-21 15:20:01,599-[TS] INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (saveAsTextFile at DMPMain.scala:17) finished in 0.109 s
2019-01-21 15:20:01,600 [main] [org.apache.spark.scheduler.DAGScheduler] [INFO] - Job 1 finished: saveAsTextFile at DMPMain.scala:17, took 0.627980 s
2019-01-21 15:20:01,600-[TS] INFO main org.apache.spark.scheduler.DAGScheduler - Job 1 finished: saveAsTextFile at DMPMain.scala:17, took 0.627980 s
2019-01-21 15:20:01,617 [Thread-1] [org.apache.spark.SparkContext] [INFO] - Invoking stop() from shutdown hook
2019-01-21 15:20:01,617-[TS] INFO Thread-1 org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2019-01-21 15:20:01,624 [Thread-1] [org.spark_project.jetty.server.AbstractConnector] [INFO] - Stopped Spark@6e9319f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-01-21 15:20:01,624-[TS] INFO Thread-1 org.spark_project.jetty.server.AbstractConnector - Stopped Spark@6e9319f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-01-21 15:20:01,626 [Thread-1] [org.apache.spark.ui.SparkUI] [INFO] - Stopped Spark web UI at http://10.0.157.188:4040
2019-01-21 15:20:01,626-[TS] INFO Thread-1 org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.157.188:4040
2019-01-21 15:20:01,634 [dispatcher-event-loop-3] [org.apache.spark.MapOutputTrackerMasterEndpoint] [INFO] - MapOutputTrackerMasterEndpoint stopped!
2019-01-21 15:20:01,634-[TS] INFO dispatcher-event-loop-3 org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2019-01-21 15:20:01,642 [Thread-1] [org.apache.spark.storage.memory.MemoryStore] [INFO] - MemoryStore cleared
2019-01-21 15:20:01,642-[TS] INFO Thread-1 org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
2019-01-21 15:20:01,643 [Thread-1] [org.apache.spark.storage.BlockManager] [INFO] - BlockManager stopped
2019-01-21 15:20:01,643-[TS] INFO Thread-1 org.apache.spark.storage.BlockManager - BlockManager stopped
2019-01-21 15:20:01,644 [Thread-1] [org.apache.spark.storage.BlockManagerMaster] [INFO] - BlockManagerMaster stopped
2019-01-21 15:20:01,644-[TS] INFO Thread-1 org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2019-01-21 15:20:01,646 [dispatcher-event-loop-0] [org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] [INFO] - OutputCommitCoordinator stopped!
2019-01-21 15:20:01,646-[TS] INFO dispatcher-event-loop-0 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2019-01-21 15:20:01,648 [Thread-1] [org.apache.spark.SparkContext] [INFO] - Successfully stopped SparkContext
2019-01-21 15:20:01,648-[TS] INFO Thread-1 org.apache.spark.SparkContext - Successfully stopped SparkContext
2019-01-21 15:20:01,649 [Thread-1] [org.apache.spark.util.ShutdownHookManager] [INFO] - Shutdown hook called
2019-01-21 15:20:01,649-[TS] INFO Thread-1 org.apache.spark.util.ShutdownHookManager - Shutdown hook called
2019-01-21 15:20:01,650 [Thread-1] [org.apache.spark.util.ShutdownHookManager] [INFO] - Deleting directory /private/var/folders/gw/182hsxpd1l78ny7x0627rr1m0000gn/T/spark-4f6b61f0-31a3-4fe1-8b41-e0ef5948f7d6
2019-01-21 15:20:01,650-[TS] INFO Thread-1 org.apache.spark.util.ShutdownHookManager - Deleting directory /private/var/folders/gw/182hsxpd1l78ny7x0627rr1m0000gn/T/spark-4f6b61f0-31a3-4fe1-8b41-e0ef5948f7d6
